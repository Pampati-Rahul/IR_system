{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bcaf36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facing issue while building the pytrec_eval python module\\nAlternatively will be using the trec evalution manually using the .exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Facing issue while building the pytrec_eval python module\n",
    "Alternatively will be using the trec evalution manually using the .exe'''\n",
    "#!pip install --upgrade setuptools\n",
    "\n",
    "#!pip install pytrec_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78160d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steps:\\n1. Import required python packages\\n2. Parse / read xml files of document and query\\n3. Pre-process the document to remove stopwords,tokens\\n4. Check for term frequency\\n5. Indexing\\n6. Inverted indexing\\n7. From the above vector formatted document, use the Vector space model for query and ranking\\n8. repat the same with BM25  retrevial model\\n9. repeat the same with some Langauge Model\\n10. eval using trec\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Steps:\n",
    "1. Import required python packages\n",
    "2. Parse / read xml files of document and query\n",
    "3. Pre-process the document to remove stopwords,tokens\n",
    "4. Check for term frequency\n",
    "5. Indexing\n",
    "6. Inverted indexing\n",
    "7. From the above vector formatted document, use the Vector space model for query and ranking\n",
    "8. repat the same with BM25  retrevial model\n",
    "9. repeat the same with some Langauge Model\n",
    "10. eval using trec\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fec467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# All imports and module installation can be found here.\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dae2b9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "junk after document element: line 24, column 0 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3369\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Input \u001b[0;32mIn [4]\u001b[0m in \u001b[0;35m<cell line: 1>\u001b[0m\n    parsed_xml = ET.parse('cranfield-trec-dataset-main/cranfield-trec-dataset-main/cran.all.1400.xml')\n",
      "  File \u001b[0;32m~\\anaconda3\\lib\\xml\\etree\\ElementTree.py:1229\u001b[0m in \u001b[0;35mparse\u001b[0m\n    tree.parse(source, parser)\n",
      "\u001b[1;36m  File \u001b[1;32m~\\anaconda3\\lib\\xml\\etree\\ElementTree.py:580\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\u001b[0m\n\u001b[1;33m    self._root = parser._parse_whole(source)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m junk after document element: line 24, column 0\n"
     ]
    }
   ],
   "source": [
    "parsed_xml = ET.parse('cranfield-trec-dataset-main/cranfield-trec-dataset-main/cran.all.1400.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e536e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Since there are parse errors, alternative solution is to read the file and add root elemet to form a valid xml'''\n",
    "\n",
    "# Common function to read the files\n",
    "def read_files_from_path(file):\n",
    "    with open(file) as f: #\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e88a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cran_all_data = read_files_from_path('cranfield-trec-dataset-main/cranfield-trec-dataset-main/cran.all.1400.xml')\n",
    "cran_document_xml = ET.fromstring('<r>' + cran_all_data + '</r>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374853b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_document_xml = ET.parse('cranfield-trec-dataset-main/cranfield-trec-dataset-main/cran.qry.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07c3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this def from previous NLP assignments and lectures\n",
    "\n",
    "def non_liginustic_preprocess(corpus):\n",
    "    corpus = str(corpus).lower() # converting to lower for normizaltion\n",
    "    corpus = re.sub('\\[.*?\\]', '', corpus) # removing brakets\n",
    "    corpus = re.sub('https?://\\S+|www\\.\\S+', '', corpus) # removing url/http\n",
    "    corpus = re.sub('[%s]' % re.escape(string.punctuation), '', corpus) # Removing the punctuation\n",
    "    corpus = re.sub('\\n', '', corpus)  # removeing next lines\n",
    "    corpus = re.sub('\\w*\\d\\w*', '', corpus) \n",
    "    corpus = re.sub('#', '', corpus) # remove hastags\n",
    "    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)   \n",
    "    stemming = PorterStemmer() \n",
    "    en_stopwords = stopwords.words(\"english\")\n",
    "    lemma_tokens = tokenizer.tokenize(corpus)\n",
    "    \n",
    "     # Removing stopwords  and apply stemming\n",
    "    preprocessed_lemma = [stemming.stem(lemma) for lemma in lemma_tokens if lemma not in en_stopwords]\n",
    "    return preprocessed_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19dd7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.Element.find\n",
    "\n",
    "document_collection = {docs.find('docno').text: [non_liginustic_preprocess(docs.find('title').text), non_liginustic_preprocess(docs.find('text').text)]\n",
    "        for docs in cran_document_xml.findall('doc')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3d28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_collection = {tags.find('num').text.strip(): [non_liginustic_preprocess(tags.find('title').text)]\n",
    "           for tags in query_document_xml.getroot().findall('top')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff31718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fecthing the uniques keys from title and texts\n",
    "unique_words_list_from_documents = list({word for key in document_collection.keys() for word in document_collection[key][0] + document_collection[key][1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b728fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trems_matrix_document = np.array([[document_collection[document_key][1].count(word) + document_collection[document_key][0].count(word) for document_key in document_collection.keys()] for word in unique_words_list_from_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d7df8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector space model\n",
    "# convert the words to vector format which acts as weights and signifes the importance.\n",
    "# Ease to cacluate using mathemtical formual and most occuring terms or words in vocalbalury.\n",
    "\n",
    "def tf_idf(docs, term_matrix):\n",
    "    num_documents = len(docs)\n",
    "    copy_term_matrix = term_matrix.copy()\n",
    "    \n",
    "#     # Calculate TF for each term-document pair\n",
    "#       # Iterate each column (documents)\n",
    "#           # calculate total no.of words in document\n",
    "#           # calculate term frequency and store in temporay term matrix\n",
    "\n",
    "    copy_term_matrix = np.array([copy_term_matrix[:, doc_column] / np.sum(term_matrix[:, doc_column]) if np.sum(term_matrix[:, doc_column]) != 0 else copy_term_matrix[:, doc_column] for doc_column in range(term_matrix.shape[1])]).T\n",
    "   \n",
    "    # using np.log for numerical stability\n",
    "    # using np.count_nonzero to get the non zero values to avoid sparsity.\n",
    "    inverse_document_frequency_matrix = np.log(num_documents / (np.count_nonzero(term_matrix, axis=1) + 1)) \n",
    "    # calculate IDF for each term\n",
    "    idf = inverse_document_frequency_matrix.reshape(len(inverse_document_frequency_matrix), 1)  # reshape\n",
    "\n",
    "    return copy_term_matrix * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc0587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vsm = tf_idf(document_collection,trems_matrix_document )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3f191a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using best of 25\n",
    "# Formula changes to calucate the BM25 and most of the core logic is same as tf-idf\n",
    "def tf_idf_bm25(docs, term_matrix, k=1.5, b=0.75):\n",
    "    avg = np.sum(np.sum(term_matrix, axis=1), axis=0) / term_matrix.shape[1]\n",
    "\n",
    "    copy_term_matrix = term_matrix.copy() \n",
    "    # comphresions making it confusing so using normal for loops, update later - TODO\n",
    "    for row in range(term_matrix.shape[0]):\n",
    "        for doc_column in range(term_matrix.shape[1]): \n",
    "            documentTotalWords = len(docs[str(doc_column + 1)][0]) + len(docs[str(doc_column + 1)][1])  # Calculate the total words in the document\n",
    "            \n",
    "            if documentTotalWords != 0:\n",
    "                copy_term_matrix[row, doc_column] = (term_matrix[row, doc_column] * (k + 1)) / (term_matrix[row, doc_column] + k * (1 - b + (b * documentTotalWords / avg)))\n",
    "            else:\n",
    "                copy_term_matrix[row, doc_column] = 0\n",
    "\n",
    "    qt = np.count_nonzero(term_matrix, axis=1)\n",
    "    idf = np.log((term_matrix.shape[1] - qt + 0.5) / (qt + 0.5))\n",
    "    idf = idf.reshape(len(idf), 1)\n",
    "\n",
    "    return copy_term_matrix * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39b68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = tf_idf_bm25(document_collection, trems_matrix_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy.linalg import norm\n",
    "# def cosine_similar(vec1, vec2):\n",
    "#     mul = norm(vec1) * norm(vec2)\n",
    "#     return 0 if mul == 0 else np.dot(vec1, vec2) / mul # check for an empty document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f325ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ranking(query, documents, vocab, ir_model, rank=50):    \n",
    "    query_vector = np.zeros(len(vocab))# Initialize query_vector with np.zeros with the length of vocab\n",
    "    for word in query: # iterating for each word in query and updating the frequency of word accordingly\n",
    "        if word in vocab:\n",
    "            query_vector[vocab.index(word)] += 1\n",
    "    query_vector = query_vector.reshape(1, -1)  # Reshape the vector\n",
    "\n",
    "    similarities = cosine_similarity(query_vector, ir_model.T).flatten()  # Transpose tfIDF for compatibility\n",
    "#     print(similarities)\n",
    "    top_rank_doc = np.argsort(similarities)[::-1][:rank]\n",
    "    \n",
    "    \n",
    "    query_rank = []\n",
    "    for rank, docno in enumerate(top_rank_doc):\n",
    "        score = {}\n",
    "        score[\"docno\"] = docno + 1\n",
    "        score[\"rank\"] = rank + 1\n",
    "        score[\"similarity\"] = similarities[docno]\n",
    "        query_rank.append(score)\n",
    "    \n",
    "    return query_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09fc77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_trec(filepath,runid,ir_model):\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "\n",
    "    with open(filepath, 'w') as output_file:\n",
    "        for q in query_collection.keys():\n",
    "            # pass one query and documents to rank with\n",
    "            results = query_ranking(query_collection[q][0], document_collection, unique_words_list_from_documents, ir_model)\n",
    "            for result in results:\n",
    "                queryId = q\n",
    "                docno = result[\"docno\"]\n",
    "                rank = result[\"rank\"]\n",
    "                similarity = result[\"similarity\"]\n",
    "                output_file.write(f\"{q} 0 {docno} {rank} {similarity} {runid}\\n\")\n",
    "        \n",
    "    command = 'trec_eval -m map -m P.5 -m ndcg cranqrel.trec.txt ' + filepath\n",
    "\n",
    "    print(subprocess.getoutput(command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf84ec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                   \tall\t0.0077\n",
      "P_5                   \tall\t0.0158\n",
      "ndcg                  \tall\t0.0259\n"
     ]
    }
   ],
   "source": [
    "eval_trec('vector_space_model_retrival.txt' , 'VSM', tf_idf_vsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c4a514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                   \tall\t0.0067\n",
      "P_5                   \tall\t0.0118\n",
      "ndcg                  \tall\t0.0231\n"
     ]
    }
   ],
   "source": [
    "eval_trec('bm25.txt' , 'BM25', bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156bc618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
